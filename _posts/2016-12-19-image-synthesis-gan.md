---
layout: post
title: Attribute based image synthesis using Generative Adversarial Networks
category: Computer Vision, Deep Learning
tags: cv, deep-learning, tensorflow
---
It's been a few months since my last post - partly in thanks to courseload keeping me busy for most of it. Now that my first semester has ended, I finally found some time to hack around tensorflow and deep learning architectures.

Couple of days back, I came across a (very recent) paper by Reed et al. titled [Generative Adversarial Text to Image Synthesis](https://arxiv.org/abs/1605.05396) which uses a generative adversarial network(GAN) to take natural language sentences and synthesize images on them. (If you need some resources on GAN, I have listed some that I found useful at the end of this post). Here are some results from their [github page](https://github.com/reedscot/icml2016)
![Text to image with GAN]({{ site.baseurl }}/public/text2image_gan1.jpg)

I decided to implement a similar GAN myself which takes in scene/object attributes and generates an image. For image attributes, I used the [SUN attribute dataset](https://cs.brown.edu/~gen/sunattributes.html) which has 102 different attributes and 14k images.

The generator network takes in a 1x102 binary vector representing which attributes are present and which are not. I decided to forgo the fully convolutional network based description compression (used in the original paper to transform the text description to a fixed dimension compressed space) since the number of attributes we have are fixed in size.

GAN Model
=========

Discriminator network
---------------------

The discriminator network essentially serves as an attribute detector. It takes in an input image and is trainedproduces a fixed length output vector(102 in our model) that encodes the presence/absence of a particular attribute as a binary vector. We resize the input images to be of size 128x128 pixels with 3 channels.

128x128x3 --[**C**]--> 64x64x64 --[**C**]--> 32x32x128 --[**C**]--> 16x16x256 --[**C**]--> 8x8x512 --[**C**]--> 4x4x1024 --[**FC**]--> 1x102

**C** denotes convolution layer  
**FC** denotes a fully connected layer

The final output binary vector is of the form $$O = [o_1, o_2, o_3, ..., o_{102}]$$ where $$o_i = 1 $$ if the $$i^{th}$$ attribute is present in the input image and $$o_i = 0$$ if it is not.


Generator network
-----------------
The generator network takes an input binary vector $$A$$ (of the same form as $$O$$ in the discriminator) and is trained to synthesize output images which contain those attributes that are set to 1 in $$I$$ and are devoid of those that are set to 0 in $$I$$. However, we de not directly feed $$A$$ to the generator. Instead, we augment it with a random noise vector $$N$$ and feed this augmented vector $$I = AN$$ to drive the generator.



Combining the generator and discriminator
-----------------------------------------
We need the generator and discriminator to compete against each other forcing the generator to improve in synthesizing valid/plausible images whilst at the same time the discriminator to get better at recognizing the attributes in an image correctly.  

The discriminator is trained on two sets of images - 
* actual ground truth images from the dataset (SUN attributes dataset)
* images generated by the generator

Before describing the networks further, let us define some terms:
* Discriminator accuracy $$S_D$$- a score lying in [-1, 1] range which indicates how accurately the discriminator detects the attributes present in the image. 
Let $$O^{\ast}$$ be the correct attribute label vector for an input image and $$O$$ be the disciriminator predicted attribute vector. One way to compute the discriminator accuracy is

$$ S_D = \frac{\sum_{i=1}^{102} 2 (o_i \equiv o^{\ast}_i) - 1}{102} $$  

Thus for each attribute correctly predicted we add $$\frac{+1}{102}$$ to the score and for each incorrectly predicted we add $$\frac{-1}{102}$$ to the score.

* Generator accuracy $$S_G$$ - a score lying in [-1, 1] range which indicates how good is the synthesized image according to the discriminator. Let $$A$$ be the input attribute vector to the generator and let $$O$$ be the discriminator ouput when it is fed the output from the generator. Then, similar to $$S_D$$ we can compute $$S_G$$ as

$$ S_G = \frac{\sum_{i=1}^{102} 2 (a_i \equiv o_i) - 1}{102} $$

The idea is that as the discriminator is trained over the real images it becomes increasingly better at detecting the attributes present in an input image. As the generator output is directly fed to the discriminator, as the discriminator improves its detection accuracy, it compels the generator to improve its generation accuracy (a measure of how good is the synthesized image).  
Although the negative of the accuracy measures described above ($$-S_D$$ and $$-S_G$$) can be directly used as loss functions for the discriminator and generator, respectively, in practice, [cross-entropy loss](http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function) works better. So we use the following loss functions:

* Discriminator loss $$ = cross\_entropy(O^{\ast}, O) $$
* Generator loss $$ = cross\_entropy(I, O)$$



Training
--------

We batch train the model using ADAM optimizer.

Conclusion
----------

The source code is available on github - [http://www.github.com/braindeadpool/image_synthesis_gan](http://www.github.com/braindeadpool/image_synthesis_gan). Its commented for most part so sifting through should be easy.


Here are some wonderful resources to get started on understanding GAN and related stuff:

* A very nice blog post by OpenAI on generative models which cover GAN - [https://openai.com/blog/generative-models/](https://openai.com/blog/generative-models/)
* A short and easy to follow blog post (with tensorflow code) - [http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/](http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/)
* The original paper by Goodfellow et al. - [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)
* A great [blog post](http://bamos.github.io/2016/08/09/deep-completion/#step-1-interpreting-images-as-samples-from-a-probability-distribution) and code by Brandon Amos


