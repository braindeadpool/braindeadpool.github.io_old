---
layout: post
title: Generative Adversarial Networks
category: Computer Vision, Deep Learning
tags: cv, deep-learning, keras
---
Having played around with GANs for a short while, I decided to write down an introductory post on it myself. So here goes!

## Generative networks

Let's start with what a generative model is. In machine learning, we have two main distinctions: generative and discriminative model. A discriminative model discriminates between the different classes of data. A generative model on the other hand doesn’t know anything about classes of data. It can generate new data which fits a particular distribution. We can take an example of a Gaussian Mixture Model. It is a generative model which, after trained on a set of points, is able to generate new random points which more-or-less fit the distribution of the training data.
The typical way to determine a generative model with training data is maximum-likelihood (MLE) , which requires marginal probabilities, most-likely estimates etc. This may be feasible when your generative model is a Gaussian Mixture Model, but not if we want to use a deep neural network as this quickly becomes intractable.

![Generator networks over years]({{ site.baseurl }}/public/mnist_gan/generative_over_years.png)
> Figure from [Ian Goodfellow's tutorial](https://arxiv.org/pdf/1701.00160.pdf)

Some commonly used generative networks are:

## Fully Visible Belief Networks

They use the chain rule of probability to decompose the probability distribution over a vector into a product over each of the members of the vector. The most popular model in this family is [PixelCNN](https://arxiv.org/abs/1606.05328). The biggest drawback with FVBNs is that the rate of generating samples is very slow. Every time you want to generate a new sample, you will have to run the model again and thus is non parallelizable.

## Variational Auto Encoders

[Variational autoencoders](https://arxiv.org/abs/1606.05908) work by marginalizing out the random variable z from the density function log p(x). Since this is intractable, it uses a variational approximation. The model wishes to maximize the lower bound on the log likelihood of the data. The main drawback here is that the model is asymptotically consistent if the distribution q is perfect. Otherwise, there would be a gap between the lower bound and actual density of the data. Another disadvantage is that the samples generated are of relatively low quality.

## Non Linear ICA

Such models begin with a simple distribution like a Gaussian and use a non-linear function to transform the distribution to another space. The main disadvantage with this is that the transformation needs to be designed to be invertible, and the latent variables must have the same dimensionality as the data. So if you want to generate 5000 pixels, you need to have 5000 latent variables.

## Boltzmann Machines

A [Boltzmann machine](https://en.wikipedia.org/wiki/Boltzmann_machine) can be defined by an energy function, and the probability of a particular state, is proportional to each of the value of the energy. To convert this to an actual probability distribution, renormalization is done by dividing the sum over the different states. This sum is intractable, which calls for approximation using Monte Carlo methods. The drawback is that these methods, especially the Markov Chain Monte Carlo methods, don’t perform well in high dimensional spaces. So although they may perform well on images like MNIST, you won’t get a similar performance on images from ImageNet.

## DCGAN

The neural networks used in GAN can be a multi level perceptron or a fractionally strided convolutional neural network (sometimes referred to as deconvolutional neural network), giving us DCGAN. As the name suggests, instead of vectorizing an image, an input vector (say random numbers generated from a uniform distribution with noise) can be converted to a image ( 64 x 64 x 3 ) by passing it through multiple fractionally strided convolutional layers.
![DCGAN]({{ site.baseurl }}/public/mnist_gan/dcgan.png)

# How do GANs work?
Adversarial training allows you to train a generative model without all of these intractable calculations. The basic idea is that you will have two adversarial models — a generator and a discriminator . The generator will be tasked with taking in a given sample from a standard random distribution and producing a point that looks sort of like it could come from the same distribution as training data. The discriminator, on the other hand, will be tasked with discriminating between samples from the true data and the artificial data generated by the generator. Each model is trying to best the other — the generator’s objective is to fool the discriminator and the discriminator’s objective is to not be fooled by the generator. In our case, both are neural nets. And what happens is that we train them both in an alternating manner. Each of their objectives can be expressed as a loss function that we can optimize via gradient descent. So we train generator for a couple steps, then train discriminator for a couple steps, then give generator the chance to improve itself, and so on. The result is that the generator and the discriminator each get better at their objectives in tandem, so that at the end, the generator is able to or is close to being able to fool the most sophisticated discriminator. In practice, this method ends up with generative neural nets that are incredibly good at producing new data (e.g. random pictures of human faces).

The generator is a differentiable function $$G$$, that has parameters that can be learned by gradient descent. The input to $$G$$ is obtained by sampling the latent vector $$z$$ from some prior distribution over latent variables. So essentially, $$z$$ is a vector of unstructured noise. $$G$$ is applied to $$z$$ to obtain a sample $$x$$ from the model, which should ideally be similar to the actual data from the train set. Like the generator, the discriminator is also a differentiable function $$D$$ that has parameters that can be learned by gradient descent. The function $$D$$, when applied to the sample $$x$$ obtained from $$G(z)$$ should ideally output a value close to zero, indicating that the sample is fake. When an actual sample from the data is fed as input to $$D$$, it should output a value close to one.
Say $$\theta_D$$ and $$\theta_G$$ are the parameters of $$D$$ and $$G$$ respectively. The discriminator would want to minimize it’s cost $$J_D(\theta_D,\theta_G)$$, but has no control over $$\theta_G$$, whereas the generator would want to minimize $$J_G(\theta_D,\theta_G)$$ without any control over $$\theta_D$$. So we need to find the Nash equilibrium values for ($$\theta_D,\theta_G$$) such that $$J_D$$ is minimum with respect to $$\theta_D$$ and $$J_G$$ is minimum with respect to $$\theta_G$$.
The below are the costs associated with the discriminator and generator :


![GAN]({{ site.baseurl }}/public/mnist_gan/gan.png)
![GAN math]({{ site.baseurl }}/public/mnist_gan/gan_math.png)
> Figures from [https://ishmaelbelghazi.github.io/ALI/](https://ishmaelbelghazi.github.io/ALI/)

# Application of GAN

* GANs can be used for umage Generation. This is the most common use case for GAN. Deep neural networks are trained to extract high-level features on natural images and images are reconstructed from the features. 
* We can generate videos using adversarial networks. Context-RNN-GAN model as explained above can be used to generate image based on it’s sequence pattern.
* Text can be converted to images. Current approaches can only roughly reflect the meaning of the given descriptions but stacked Generative Adversarial Networks (StackGAN) can be used to generate photo-realistic images conditioned on text descriptions.
* PPGNs are used to produce high resolution image generation. They are basically an approximate Langevin sampling approach to generating images with a Markov chain. The gradients for the Langevin sampler are estimated using a denoising autoencoder. The denoising autoencoder is trained with several losses, including a GAN loss.
* We can generate images interactively! iGAN uses a GAN to produce the most similar realistic image by automatically adjusting the output keeping all edits as realistic as possible.
* GANs can be used for diagrammatic abstract reasoning. When both the generator and the discriminator are based on contextual history (modeled as RNNs) (Context-RNN-GAN model) then the adversarial discriminator guides the generator to produce realistic images for the particular time step in the image sequence. This can be used for video generation.


# Toy application - a DCGAN-like model trained on MNIST

Inspired by the DCGAN architecture, I implemented a simple GAN model that is trained on [MNIST](http://yann.lecun.com/exdb/mnist/) database of handwritten digits.

Generator network
---------------------

We start with a simple generator model that takes a noise vector of 100 dimensions that theoretically represents our latent source distribution. Here's the model representation:
![Generator model]({{ site.baseurl }}/public/mnist_gan/generator_model.png)
In keras, you can code it up quickly using the sequential api as:
{% highlight python %}
generator = Sequential()
    generator.add(Dense(input_dim=noise_vector_dim, output_dim=1024))
    generator.add(Activation('relu'))
    generator.add(Dense(128 * 7 * 7))
    generator.add(BatchNormalization())
    generator.add(Activation('relu'))
    generator.add(Reshape((128, 7, 7), input_shape=(128 * 7 * 7,)))
    generator.add(UpSampling2D(size=(2, 2), dim_ordering="th"))
    generator.add(Convolution2D(64, 5, 5, border_mode='same', dim_ordering="th"))
    generator.add(Activation('relu'))
    generator.add(UpSampling2D(size=(2, 2), dim_ordering="th"))
    generator.add(Convolution2D(1, 5, 5, border_mode='same', dim_ordering="th"))
    generator.add(Activation('relu'))
{% endhighlight %}


Discriminator network
---------------------
The discriminator uses some tips from the DCGAN paper and the original GAN paper. It takes in a 28x28 grayscale image and predicts whether the image is synthetic or real. Here's the model representation and code in keras:
![Discriminator model]({{ site.baseurl }}/public/mnist_gan/discriminator_model.png)
{% highlight python %}
discriminator = Sequential()
    discriminator.add(Convolution2D(64, 5, 5,
                                    border_mode='same',
                                    input_shape=(1, 28, 28),
                                    dim_ordering="th"))
    discriminator.add(Activation('tanh'))
    discriminator.add(MaxPooling2D(pool_size=(2, 2), dim_ordering="th"))
    discriminator.add(Convolution2D(128, 5, 5, border_mode='same', dim_ordering="th"))
    discriminator.add(Activation('tanh'))
    discriminator.add(MaxPooling2D(pool_size=(2, 2), dim_ordering="th"))
    discriminator.add(Flatten())
    discriminator.add(Dense(1024))
    discriminator.add(Activation('tanh'))
    discriminator.add(Dense(1))
    discriminator.add(Activation('sigmoid'))
{% endhighlight %}

Combining the generator and discriminator
-----------------------------------------
We need the generator and discriminator to compete against each other forcing the generator to improve in synthesizing plausible images whilst at the same time the discriminator to get better at recognizing the if an image is real or fake.

{% highlight python %}
# define gan model by connecting generator output to discriminator input
    gan = Sequential()
    gan.add(generator)
    gan.add(discriminator)
{% endhighlight %}

The discriminator is trained on two sets of images - 
* actual ground truth images from the dataset (MNIST attributes dataset)
* synthetic images generated by the generator

The idea is that as the discriminator is trained over the real images it becomes increasingly better at detecting the authenticity of an input image. Since the generator output is directly fed to the discriminator, as the discriminator improves its detection accuracy, it compels the generator to improve its realism. The generator attempts to match the distribution from which the images are being generated and replicates images from it.  
We use [binary cross-entropy loss](http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function) to compute the loss on both the discriminator and GAN model and use SGD for optimizing both the models.

NOTE: We freeze the discriminator while training the GAN and then unfreeze it to train it alone. Alternating cycles of such training, results in the generator and discriminator playing against each other.


Results
-------
I trained the model for 100 epochs with a batch size of 256. Training was done on [Floydhub](http://www.floydhub.com) [a wonderful cloud DL service that I recommend - plus, they give 100 hours of GPU time for free on signup!!] using a Tesla K80 for ~3 hours.

Here are some results after certain epochs:

* Epoch 1 - ![Epoch 1]({{ site.baseurl }}/public/mnist_gan/image_1_200.png)
* Epoch 10 - ![Epoch 10]({{ site.baseurl }}/public/mnist_gan/image_10_200.png)
> We are starting to see some patterns here
* Epoch 20 - ![Epoch 20]({{ site.baseurl }}/public/mnist_gan/image_20_200.png)
* Epoch 30 - ![Epoch 30]({{ site.baseurl }}/public/mnist_gan/image_30_200.png)
> Still noisy, but some digit like shapes are quite legible
* Epoch 40 - ![Epoch 40]({{ site.baseurl }}/public/mnist_gan/image_40_200.png)
* Epoch 60 - ![Epoch 60]({{ site.baseurl }}/public/mnist_gan/image_60_200.png)
* Epoch 80 - ![Epoch 80]({{ site.baseurl }}/public/mnist_gan/image_80_200.png)
* Epoch 100 - ![Epoch 100]({{ site.baseurl }}/public/mnist_gan/image_100_200.png)
> Now the digits seem more realistic and quite noise-free

The training losses look like this-
![Training losses]({{ site.baseurl }}/public/mnist_gan/training_losses.png)
> Figure shows binary cross entropy loss of discriminator vs generator-discriminator model across batches (blue = discriminator, green = generator-discriminator)


Future extensions
-----------------
### Conditional GANs

The model above can be modified into a conditional GAN by adding two additonal input layers. A one-hot encoding representing the training input label can be concatenated with the generator output and fed to the discriminator. Similarly, the noise input vector should also be concatenated with the one-hot encoding. This lets you now condition the generator and discriminator on an auxiliary distribution - in our toy model, it would be the one-hot encoding representing the label of digit we wish to generate/discriminate.

![ACGAN]({{ site.baseurl }}/public/mnist_gan/acgan.png)
> Figure from [ACGAN](https://arxiv.org/abs/1610.09585)

### Training tweaks
[Generative Adversarial Text to Image Synthesis](https://arxiv.org/abs/1605.05396) generates images conditioned on a text embedding allowing them to generate images conditioned on a caption. Here's the [presentation](https://goo.gl/jSr46h) I used for my [Adv. Computer Vision class 2017](www.cc.gatech.edu/~hays/7476/) which briefly summarize their methods and results. An intuitive and important training technique they use is manifold interpolation.
Manifold interpolation creates additional training samples for the generator by interpolating the noise vector corresponding to randomly sampled pair of conditional inputs. This makes the data manifold between those two conditional inputs more dense and enables the generator to learn better. 

Another important technique is the concept of using three pairs of training inputs instead of two - 
* (real image, incorrect conditional input) - negative sample
* (real image, correct conditional input) - positive sample
* (fake image, correct conditional input) - negative sample

This eases the GAN training as it no longer needs to implicitly separate the two sources of error - real image with incorrect conditional input and fake image with correct conditional input. By directly providing them as training samples, the GAN training dynamics improve a lot.

Conclusion
----------
The source code is available on github - [https://github.com/braindeadpool/gan_fh](https://github.com/braindeadpool/gan_fh) - along with instructions to run it on Floydhub.

Here are some wonderful resources to get started on understanding GAN and related stuff:

* A very nice blog post by OpenAI on generative models which cover GAN - [https://openai.com/blog/generative-models/](https://openai.com/blog/generative-models/)
* A short and easy to follow blog post (with tensorflow code) - [http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/](http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/)
* The original paper by Goodfellow et al. - [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) and the DCGAN paper [Unsupervised representation learning with deep convolutional generative adversarial networks](https://arxiv.org/pdf/1511.06434.pdf)
* Practical tips on training GANS - [https://github.com/soumith/ganhacks](https://github.com/soumith/ganhacks)
* A great [blog post](http://bamos.github.io/2016/08/09/deep-completion/#step-1-interpreting-images-as-samples-from-a-probability-distribution) and code by Brandon Amos


In short, training GANS are hard and tricky. However, a lot of work is being published currently on understanding and training GANs easier. In addition, lot of them show really cool applications. Here are some select projects showcasing them:

* Unpaired Image-to-image translation using Cycle-consistent Adversarial Networks - [https://junyanz.github.io/CycleGAN/](https://junyanz.github.io/CycleGAN/)
* [f-GAN](http://papers.nips.cc/paper/6066-f-gan-training-generative-neural-samplers-using-variational-divergence-minimization)
* [Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028)
* Image-to-Image Translation with Conditional Adversarial Nets - [Pix2pix](https://phillipi.github.io/pix2pix/)
